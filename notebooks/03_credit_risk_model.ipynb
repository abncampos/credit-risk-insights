{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e0e58b",
   "metadata": {},
   "source": [
    "# 03 - Credit Risk Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8199ba98",
   "metadata": {},
   "source": [
    "This notebook builds baseline predictive models for credit default using the processed German Credit dataset. We will train Logistic Regression and Random Forest models, evaluate metrics, and produce professional insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722025d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data/processed')\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc6d8b",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned German Credit Data (My Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DATA_DIR / 'german_credit_clean.csv'\n",
    "if df.exists():\n",
    "    credit_data_model = pd.read_csv(df)\n",
    "    print('Loaded processed German Credit dataset:', credit_data_model.shape)\n",
    "else:\n",
    "    print('Cleaned dataset not found. Please run 02_exploratory_analysis.ipynb first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd934171",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing (Steps I Applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'credit_data_model' in locals():\n",
    "    # Example: assuming 'default' is target, encode categoricals\n",
    "    target = 'default'\n",
    "    X = credit_data_model.drop(columns=[target])\n",
    "    y = credit_data_model[target]\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edfb7c",
   "metadata": {},
   "source": [
    "## 3. Split Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X' in locals():\n",
    "    X_train_data, X_test_data, y_train_data, y_test_data = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print('Training set:', X_train_data.shape, 'Test set:', X_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3282b80",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train_data' in locals():\n",
    "    lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LogisticRegression(max_iter=1000))])\n",
    "    lr_pipeline.fit(X_train_data, y_train_data)\n",
    "    y_pred_lr = lr_pipeline.predict(X_test_data)\n",
    "    y_prob_lr = lr_pipeline.predict_proba(X_test_data)[:,1]\n",
    "    print('Logistic Regression Classification Report:')\n",
    "    print(classification_report(y_test_data, y_pred_lr))\n",
    "    print('ROC-AUC:', roc_auc_score(y_test_data, y_prob_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462fa79",
   "metadata": {},
   "source": [
    "## 5. Random Forest Model & Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train_data' in locals():\n",
    "    rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "    rf_pipeline.fit(X_train_data, y_train_data)\n",
    "    y_pred_rf = rf_pipeline.predict(X_test_data)\n",
    "    y_prob_rf = rf_pipeline.predict_proba(X_test_data)[:,1]\n",
    "    print('Random Forest Classification Report:')\n",
    "    print(classification_report(y_test_data, y_pred_rf))\n",
    "    print('ROC-AUC:', roc_auc_score(y_test_data, y_prob_rf))\n",
    "    # Feature importance example\n",
    "    rf_model = rf_pipeline.named_steps['classifier']\n",
    "    if hasattr(rf_model, 'feature_importances_'):\n",
    "        importance = rf_model.feature_importances_\n",
    "        print('Random Forest feature importances calculated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77adf298",
   "metadata": {},
   "source": [
    "## 6. My Insights and Interpretation\n",
    "- Compare Logistic Regression vs Random Forest performance.\n",
    "- Highlight important predictors of default.\n",
    "- Discuss implications for credit risk management."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
